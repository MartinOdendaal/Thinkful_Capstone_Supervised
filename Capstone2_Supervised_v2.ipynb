{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2: Supervised learning on 20 newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective 1:** Compare the performance of various models classifying text-based models. Performance is defined as run time, accuracy, recall, precision and f1 scores.\n",
    "\n",
    "**Objective 2:** Apply these learnings to a data held by an editing firm to test their current classification performance. These results will not be shown here.\n",
    "\n",
    "**The dataset:** I use the 20 newsgroups dataset. It is a text-based dataset built into the scikit learn library comprises around 18000 newsgroups posts on 20 topics. The main page and instructions for downloading can be found [here]( http://scikit-learn.org/stable/datasets/twenty_newsgroups.html). This dataset does have several [tutorials](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) and [examples]( http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html). These resources were used as a good introduction, but I made several adjustments (how the data was processed and the classifiers that I used).\n",
    "\n",
    "**What I learnt:** Through this project I improved my ability and understanding of text processing and I learnt two new models (xgboost and stochastic gradient descent) and how to implement these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run time  Accuracy  Precision  Recall    f1\n",
      "nb       0.03      0.82       0.82    0.82  0.82\n",
      "SVC     19.06      0.78       0.79    0.78  0.79\n",
      "XGC     19.53      0.74       0.75    0.74  0.74\n",
      "SGD      0.18      0.81       0.81    0.81  0.81\n"
     ]
    }
   ],
   "source": [
    "# Reproduced from conclusion\n",
    "print(summary.round(2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the Naïve Bayes classifier performed best. A more hands-on approach to the text cleaning and optimizing the models only led to modest gains against the external benchmark based solely on the scikit learn library (see below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An external baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the methodology on scikit learn’s [website]( http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) provides a good external baseline to measure performance against. This is reproduced below. Note that in loading the data headers, footers, and quotes are excluded since in the final use of this model (objective 2) this type of information may not necessarily be available.  Excluding this information reduces the predictive ability of the model. If this is not excluded ‘edu’ and ‘gov’ (from email addresses) become important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic gradient descent accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Note categories are changed from example to match categories below\n",
    "categories = ['comp.sys.mac.hardware','comp.windows.x',\n",
    "              'misc.forsale', 'rec.autos','rec.sport.baseball', 'rec.sport.hockey',\n",
    "              'sci.crypt','sci.electronics']\n",
    "\n",
    "# Note, headers, footers and quotes are removed\n",
    "twenty_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "\n",
    "# Naive Bayes\n",
    "# Create pipeline to process and create model instance\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])\n",
    "# Fit\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "\n",
    "#Metric\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print('Naive Bayes accuracy: %0.3f' %np.mean(predicted == twenty_test.target))\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "# Create pipeline to process and create model instance\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42))])\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "predicted = text_clf.predict(docs_test)\n",
    "print('Stochastic gradient descent accuracy: %0.3f' % np.mean(predicted == twenty_test.target)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline \n",
    "\n",
    "The outline for this project is:\n",
    "1. Setting up the data: Clean and process the text\n",
    "2. Create vectors based on term frequencies (weighted with inverse document frequencies)\n",
    "3. Compare the results of baseline and optimized (through Gridsearch) models <br>\n",
    "    3.1 Naïve Bayes Classifier <br>\n",
    "    3.2 Support Vector Classifier <br>\n",
    "    3.3 Extreme Gradient Boosting classifier (xbgoost) <br>\n",
    "    3.4 Stochastic Gradient Descent Classifier <br>\n",
    "4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Full dataset categories\n",
    "categories = ['comp.graphics','comp.os.ms-windows.misc',\n",
    "                  'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "                  'comp.windows.x', 'rec.autos','rec.motorcycles',\n",
    "                  'rec.sport.baseball','rec.sport.hockey', 'sci.crypt',\n",
    "                  'sci.electronics','sci.med','sci.space',\n",
    "                  'misc.forsale', 'talk.politics.misc',\n",
    "                  'talk.politics.guns','talk.politics.mideast', 'talk.religion.misc',\n",
    "                  'alt.atheism','soc.religion.christian']\n",
    "\n",
    "# Reduce size of dataset to improve speed\n",
    "random.seed(13)\n",
    "categories_small = random.sample(categories, 8)\n",
    "\n",
    "# Import dataset (type Bunch)\n",
    "# Remove headers, footers, and quotes so that classifiers only work on text\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories_small,\n",
    "                             remove=('headers', 'footers', 'quotes'),\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories_small,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "# Also import full dataset for cv\n",
    "dataset_full = fetch_20newsgroups(subset='all', categories=categories_small,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to dataframe\n",
    "news = pd.DataFrame(dataset.data, columns=['Text'])\n",
    "news_test = pd.DataFrame(dataset_test.data, columns=['Text'])\n",
    "news_full = pd.DataFrame(dataset_full.data, columns=['Text'])\n",
    "\n",
    "# Set outcome variable\n",
    "y = dataset.target\n",
    "y_test = dataset_test.target\n",
    "y_full = dataset_full.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 597, 5: 600, 6: 595, 7: 591, 1: 593, 3: 594, 0: 578, 2: 585}\n"
     ]
    }
   ],
   "source": [
    "# Check for class balance\n",
    "\n",
    "def targetcounts(alist):\n",
    "    '''Count of items in list\n",
    "    Returns dictionary containing count'''\n",
    "    adict = {}\n",
    "    for key in alist:\n",
    "        try:\n",
    "            adict[key] += 1\n",
    "        except KeyError:\n",
    "            adict[key] = 1\n",
    "    return adict \n",
    "\n",
    "counts = targetcounts(dataset.target)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def textcleaner(text):\n",
    "    ''' Takes in raw unformatted text and strips punctuation, removes whitespace,\n",
    "    strips numbers, tokenizes and stems.\n",
    "    Returns string of processed text to be used into CountVectorizer\n",
    "    '''\n",
    "    # Lowercase and strip everything except words\n",
    "    cleaner = re.sub(r\"[^a-zA-Z ]+\", ' ', text.lower())\n",
    "    # Tokenize\n",
    "    cleaner = word_tokenize(cleaner)\n",
    "    ps = PorterStemmer()\n",
    "    clean = []\n",
    "    for w in cleaner:\n",
    "        # filter out stopwords\n",
    "        if w not in stopWords:\n",
    "            # filter out short words\n",
    "            if len(w)>2:\n",
    "                # Stem \n",
    "                clean.append(ps.stem(w))\n",
    "    return ' '.join(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done news\n",
      "Done test\n"
     ]
    }
   ],
   "source": [
    "# Running cleaning function on train and test df\n",
    "news['Clean_text'] = news.Text.apply(lambda x: textcleaner(x))\n",
    "print('Done news')\n",
    "news_test['Clean_text'] = news_test.Text.apply(lambda x: textcleaner(x))\n",
    "print('Done test')\n",
    "# On full dataset\n",
    "news_full['Clean_text'] = news_full.Text.apply(lambda x: textcleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unprocessed text\n",
    "news.drop(['Text'], inplace=True, axis=1)\n",
    "news_test.drop(['Text'], inplace=True, axis=1)\n",
    "news_full.drop(['Text'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "S_news = news.loc[:,:]\n",
    "S_news['y'] = y\n",
    "S_news_test = news_test.loc[:,:]\n",
    "S_news_test['y'] = y_test\n",
    "S_news_full = news_full.loc[:,:]\n",
    "S_news_full['y'] = y_full\n",
    "\n",
    "S_news.to_csv(r'C:\\Users\\User\\Documents\\Python_scripts\\Thinkful\\news.csv')\n",
    "S_news_test.to_csv(r'C:\\Users\\User\\Documents\\Python_scripts\\Thinkful\\news_test.csv')\n",
    "S_news_full.to_csv(r'C:\\Users\\User\\Documents\\Python_scripts\\Thinkful\\news_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unhash to reload\n",
    "#news = pd.read_csv(r'C:\\Users\\User\\Documents\\Python_scripts\\Thinkful\\news.csv', encoding='latin-1')\n",
    "#news.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "#news_test = pd.read_csv(r'C:\\Users\\User\\Documents\\Python_scripts\\Thinkful\\news_test.csv', encoding='latin-1')\n",
    "#news_test.drop('Unnamed: 0', inplace=True, axis=1)\n",
    "\n",
    "# Drop na\n",
    "#news.dropna(inplace=True)\n",
    "#news_test.dropna(inplace=True)\n",
    "\n",
    "#Split out y\n",
    "#y = news.loc[:, 'y']\n",
    "#y_test = news.loc[:,'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf matrix on train data\n",
    "vectorizer = TfidfVectorizer(min_df=6, strip_accents='ascii', analyzer='word', lowercase=True,\n",
    "                             ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(news.loc[:, 'Clean_text'])\n",
    "\n",
    "# Apply model to test data\n",
    "X_test = vectorizer.transform(news_test.loc[:, 'Clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4733, 8474)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seagat', 'run sun', 'end user', 'salesman', 'sale price', 'enforc prohibit', 'safeti effect', 'sacrific', 'enough make', 'rychel', 'entiti respons', 'entitl unbreak', 'russel', 'entropi', 'runtim', 'run wire', 'run win', 'end get', 'sampl program', 'end car', 'encrypt busi', 'say someth', 'say one', 'say got', 'say get', 'say everi', 'say could', 'encrypt clipper', 'encrypt unit', 'encrypt differ', 'encrypt file', 'encrypt need', 'encrypt polici', 'sand', 'encrypt threaten', 'run unix', 'equil', 'serial line', 'er', 'event mask', 'ron franci', 'ever get', 'ever sinc', 'role infrastructur', 'exampl say', 'exce', 'expect discuss', 'expect see', 'expedit', 'expert outsid', 'explor new', 'righti', 'right want', 'right reserv', 'even without', 'even best', 'rot', 'rsa data', 'erickson', 'run game', 'error code', 'error fail', 'esa tikkanen', 'rsa patent', 'royalti', 'evalu algorithm', 'especi sinc', 'essensa', 'etc know', 'round pick', 'euro', 'rotten', 'employ voic', 'email thank', 'email repli', 'schmidt', 'dykstra', 'selector', 'earli season', 'seem use', 'seem think', 'seem recal', 'seem pretti', 'easi break', 'seem bit', 'see post', 'easili today', 'east boston', 'secur phone', 'secur mani', 'eavesdropp', 'selk', 'duti cycl', 'sell make', 'sendmail', 'dribbl', 'drive also', 'separ order', 'drive car', 'sep']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "features = vectorizer.get_feature_names()\n",
    "top_n = 100\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To look at score by sample\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "doc = 1\n",
    "feature_index = X_train[doc,:].nonzero()[1]\n",
    "tfidf_scores = zip(feature_index, [X_train[doc, x] for x in feature_index])\n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print(w, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes Classifier\n",
    "\n",
    "The multinomial Naive Bayes classifier is a good baseline for classification with discrete features (in this case a word count). While the multinomial distribution normally requires integer feature counts, according to [Scikitlearn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html), fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.032\n",
      "Accuracy is: 0.811\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.77      0.80      0.78       385\n",
      "       comp.windows.x       0.86      0.88      0.87       395\n",
      "         misc.forsale       0.84      0.81      0.83       390\n",
      "            rec.autos       0.83      0.79      0.81       396\n",
      "   rec.sport.baseball       0.92      0.83      0.87       397\n",
      "     rec.sport.hockey       0.73      0.92      0.82       399\n",
      "            sci.crypt       0.78      0.85      0.81       396\n",
      "      sci.electronics       0.79      0.61      0.68       393\n",
      "\n",
      "          avg / total       0.81      0.81      0.81      3151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Start timing\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#Initialize and fit\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = nb.predict(X_test)\n",
    "\n",
    "# Stop timing\n",
    "stop = timeit.default_timer()\n",
    "nb_time = stop-start\n",
    "print(\"Run time: %0.3f\" % (nb_time))\n",
    "\n",
    "# Showing model performance\n",
    "print(\"Accuracy is: %0.3f\" % nb.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.mac.hardware: monitor get one card simm use problem drive appl mac\n",
      "comp.windows.x: applic display file run program motif widget server use window\n",
      "misc.forsale: use pleas price condit new includ sell ship offer sale\n",
      "rec.autos: good look dealer drive one engin get would like car\n",
      "rec.sport.baseball: win think run basebal player hit pitch team game year\n",
      "rec.sport.hockey: would playoff nhl year season player hockey play team game\n",
      "sci.crypt: peopl system would secur use govern clipper chip encrypt key\n",
      "sci.electronics: work get know like power anyon circuit would one use\n"
     ]
    }
   ],
   "source": [
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
    "        \n",
    "show_top10(nb, vectorizer, dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "Accuracy is: 0.817\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.77      0.79      0.78       385\n",
      "       comp.windows.x       0.90      0.87      0.89       395\n",
      "         misc.forsale       0.84      0.81      0.83       390\n",
      "            rec.autos       0.80      0.81      0.81       396\n",
      "   rec.sport.baseball       0.91      0.86      0.88       397\n",
      "     rec.sport.hockey       0.74      0.92      0.82       399\n",
      "            sci.crypt       0.84      0.82      0.83       396\n",
      "      sci.electronics       0.76      0.65      0.70       393\n",
      "\n",
      "          avg / total       0.82      0.82      0.82      3151\n",
      "\n",
      "col_0    0    1    2    3    4    5    6    7\n",
      "row_0                                        \n",
      "0      306   10   26    6    1    1   10   39\n",
      "1        8  345    2    1    5    3    9   11\n",
      "2       19    6  315   11    2    1    3   16\n",
      "3        7    7   16  320    5    7    9   27\n",
      "4        1    1    6    4  340   11    6    4\n",
      "5       15    9   11   32   33  368   18   13\n",
      "6        6   10    2    6    6    6  323   26\n",
      "7       23    7   12   16    5    2   18  257\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "params = {'alpha': [0.01, 0.1, 0.3, 0.4, 0.5, 0.75, 1]}\n",
    "\n",
    "# Initialize the model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Apply GridSearch to the model\n",
    "grid = model_selection.GridSearchCV(nb, params)\n",
    "\n",
    "# Fit to data\n",
    "grid.fit(X_train, y)\n",
    "\n",
    "# For use in CV later\n",
    "nb_best = grid.best_estimator_\n",
    "\n",
    "# Metrics \n",
    "print(grid.best_estimator_)\n",
    "print(\"Accuracy is: %0.3f\" % grid.score(X_test, y_test))\n",
    "y_hat = grid.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Summary stats\n",
    "# Time, accuracy, precision, recall, f1\n",
    "compareModels = {}\n",
    "compareModels['nb'] = [nb_time, metrics.accuracy_score(y_test, y_hat),\n",
    "                     metrics.precision_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.recall_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.f1_score(y_test, y_hat, average = 'macro')]                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for iteration 0 is 0.8538812785388128\n",
      "Score for iteration 1 is 0.8622526636225266\n",
      "Score for iteration 2 is 0.8508371385083714\n",
      "Score for iteration 3 is 0.8356164383561644\n",
      "Score for iteration 4 is 0.8493150684931506\n",
      "Score for iteration 5 is 0.8432267884322678\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Calculating CV using KFolds\n",
    "count = 0\n",
    "kf = model_selection.KFold(n_splits=6, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(news_full, y_full):    \n",
    "    Xk_train = vectorizer.fit_transform(news_full.iloc[train_index,0])\n",
    "    Xk_test =  vectorizer.transform(news_full.iloc[test_index, 0])\n",
    "    yk_train, yk_test = y_full[train_index], y_full[test_index]\n",
    "    # Create instance based on GridCV\n",
    "    nb = nb_best\n",
    "    nb.fit(Xk_train, yk_train)\n",
    "    print('Score for iteration {} is {}'.format(count, nb.score(Xk_test, yk_test)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Support Vector Machine Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time:19.060\n",
      "Accuracy is: 0.784\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.74      0.74      0.74       385\n",
      "       comp.windows.x       0.90      0.82      0.86       395\n",
      "         misc.forsale       0.83      0.80      0.81       390\n",
      "            rec.autos       0.64      0.84      0.73       396\n",
      "   rec.sport.baseball       0.81      0.81      0.81       397\n",
      "     rec.sport.hockey       0.87      0.84      0.86       399\n",
      "            sci.crypt       0.88      0.75      0.81       396\n",
      "      sci.electronics       0.68      0.66      0.67       393\n",
      "\n",
      "          avg / total       0.79      0.78      0.79      3151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Start timing\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Create instance and fit\n",
    "sv = SVC(kernel='linear')\n",
    "sv.fit(X_train, y)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = sv.predict(X_test)\n",
    "\n",
    "# Stop timing\n",
    "stop = timeit.default_timer()\n",
    "sv_time = stop - start\n",
    "print(\"Run time:%0.3f\" %sv_time)\n",
    "\n",
    "# Showing model performance\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(\"Accuracy is: %0.3f\" % sv.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight='balanced', coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Accuracy is: 0.784\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.74      0.75      0.74       385\n",
      "       comp.windows.x       0.90      0.82      0.86       395\n",
      "         misc.forsale       0.83      0.80      0.81       390\n",
      "            rec.autos       0.64      0.84      0.72       396\n",
      "   rec.sport.baseball       0.81      0.81      0.81       397\n",
      "     rec.sport.hockey       0.87      0.84      0.86       399\n",
      "            sci.crypt       0.88      0.75      0.81       396\n",
      "      sci.electronics       0.68      0.66      0.67       393\n",
      "\n",
      "          avg / total       0.79      0.78      0.79      3151\n",
      "\n",
      "col_0    0    1    2    3    4    5    6    7\n",
      "row_0                                        \n",
      "0      287   15   31    4    4    1    9   39\n",
      "1        7  325    3    3    3    2    5   14\n",
      "2       17    4  312   11    5    1    8   20\n",
      "3       26   17   20  333   33   25   33   37\n",
      "4        3    7   10   11  321   22    9   11\n",
      "5        6    0    2    9   24  336    8    0\n",
      "6        5    9    1    2    4    6  298   14\n",
      "7       34   18   11   23    3    6   26  258\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.1, 1, 10],\n",
    "          'kernel': ['linear'],\n",
    "          'class_weight': ['balanced', None]}\n",
    "\n",
    "# Initialize the model\n",
    "sv = SVC()\n",
    "\n",
    "# Apply GridSearch to the model\n",
    "grid = model_selection.GridSearchCV(sv, params)\n",
    "grid.fit(X_train, y)\n",
    "\n",
    "# Save model for use in CV\n",
    "sv_best = grid.best_estimator_\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "print(\"Accuracy is: %0.3f\" % grid.score(X_test, y_test))\n",
    "\n",
    "y_hat = grid.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "# Time, accuracy, precision, recall, f1\n",
    "compareModels['SVC'] = [sv_time, metrics.accuracy_score(y_test, y_hat),\n",
    "                     metrics.precision_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.recall_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.f1_score(y_test, y_hat, average = 'macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for iteration 0 is 0.8135464231354642\n",
      "Score for iteration 1 is 0.8401826484018264\n",
      "Score for iteration 2 is 0.8401826484018264\n",
      "Score for iteration 3 is 0.821917808219178\n",
      "Score for iteration 4 is 0.845509893455099\n",
      "Score for iteration 5 is 0.837138508371385\n"
     ]
    }
   ],
   "source": [
    "# Calculating CV using KFolds\n",
    "count = 0\n",
    "kf = model_selection.KFold(n_splits=6, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(news_full, y_full):    \n",
    "    Xk_train = vectorizer.fit_transform(news_full.iloc[train_index,0])\n",
    "    Xk_test =  vectorizer.transform(news_full.iloc[test_index, 0])\n",
    "    yk_train, yk_test = y_full[train_index], y_full[test_index]\n",
    "    # Create instance based on GridCV\n",
    "    sv = sv_best\n",
    "    sv.fit(Xk_train, yk_train)\n",
    "    print('Score for iteration {} is {}'.format(count, sv.score(Xk_test, yk_test)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Extreme gradient boosting (xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 19.526\n",
      "Accuracy is: 0.705\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.76      0.69      0.72       385\n",
      "       comp.windows.x       0.87      0.73      0.80       395\n",
      "         misc.forsale       0.77      0.72      0.74       390\n",
      "            rec.autos       0.80      0.63      0.70       396\n",
      "   rec.sport.baseball       0.49      0.82      0.61       397\n",
      "     rec.sport.hockey       0.84      0.78      0.81       399\n",
      "            sci.crypt       0.86      0.67      0.75       396\n",
      "      sci.electronics       0.53      0.60      0.56       393\n",
      "\n",
      "          avg / total       0.74      0.71      0.71      3151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "# Start timing\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Initialize\n",
    "xgc = xgb.XGBClassifier(objective= 'multi:softprob')\n",
    "\n",
    "#Fit training \n",
    "xgc.fit(X_train, y)\n",
    "\n",
    "# Apply to test\n",
    "y_hat = xgc.predict(X_test)\n",
    "\n",
    "# Stop timing\n",
    "stop = timeit.default_timer()\n",
    "xgc_time = stop - start \n",
    "print(\"Run time: %0.3f\" % xgc_time)\n",
    "\n",
    "# Showing model performance\n",
    "print(\"Accuracy is: %0.3f\" % xgc.score(X_test, y_test))\n",
    "\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=1000, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.743 \n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.75      0.73      0.74       385\n",
      "       comp.windows.x       0.85      0.80      0.82       395\n",
      "         misc.forsale       0.78      0.77      0.78       390\n",
      "            rec.autos       0.72      0.69      0.71       396\n",
      "   rec.sport.baseball       0.62      0.83      0.71       397\n",
      "     rec.sport.hockey       0.82      0.79      0.81       399\n",
      "            sci.crypt       0.84      0.73      0.78       396\n",
      "      sci.electronics       0.62      0.60      0.61       393\n",
      "\n",
      "          avg / total       0.75      0.74      0.74      3151\n",
      "\n",
      "col_0    0    1    2    3    4    5    6    7\n",
      "row_0                                        \n",
      "0      281   15   26    7    3    3    9   30\n",
      "1       12  315    5    5    2    3    9   21\n",
      "2       16   15  302   16    5    4    9   19\n",
      "3       14    3   17  273   12    9   12   37\n",
      "4       23   14   22   44  330   52   23   22\n",
      "5        4    7    3    8   32  316    9    7\n",
      "6       11    6    4    5    4    6  289   21\n",
      "7       24   20   11   38    9    6   36  236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "params = {'objective': ['multi:softprob'],\n",
    "          'n_estimators': [100, 1000],\n",
    "          #'gamma': [0, 0.001],\n",
    "          #'reg_alpha': [0,1]\n",
    "         }\n",
    "\n",
    "# Initialize the model\n",
    "xgc = xgb.XGBClassifier()\n",
    "\n",
    "# Apply GridSearch to the model\n",
    "grid = model_selection.GridSearchCV(xgc, params)\n",
    "grid.fit(X_train, y)\n",
    "\n",
    "xgc_best = grid.best_estimator_\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "print(\"Accuracy is %0.3f \" % grid.score(X_test, y_test))\n",
    "\n",
    "y_hat = grid.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "# Time, accuracy, precision, recall, f1\n",
    "compareModels['XGC'] = [xgc_time, metrics.accuracy_score(y_test, y_hat),\n",
    "                     metrics.precision_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.recall_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.f1_score(y_test, y_hat, average = 'macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for iteration 0 is 0.834855403348554\n",
      "Score for iteration 1 is 0.8333333333333334\n",
      "Score for iteration 2 is 0.8234398782343988\n",
      "Score for iteration 3 is 0.850076103500761\n",
      "Score for iteration 4 is 0.8287671232876712\n",
      "Score for iteration 5 is 0.8287671232876712\n"
     ]
    }
   ],
   "source": [
    "# Calculating CV using KFolds\n",
    "count = 0\n",
    "kf = model_selection.KFold(n_splits=6, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(news_full, y_full):    \n",
    "    Xk_train = vectorizer.fit_transform(news_full.iloc[train_index,0])\n",
    "    Xk_test =  vectorizer.transform(news_full.iloc[test_index, 0])\n",
    "    yk_train, yk_test = y_full[train_index], y_full[test_index]\n",
    "    # Create instance based on GridCV\n",
    "    xgc = sv_best\n",
    "    xgc.fit(Xk_train, yk_train)\n",
    "    print('Score for iteration {} is {}'.format(count, xgc.score(Xk_test, yk_test)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time: 0.178\n",
      "Accuracy is: 0.789\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.73      0.77      0.75       385\n",
      "       comp.windows.x       0.90      0.84      0.87       395\n",
      "         misc.forsale       0.78      0.83      0.80       390\n",
      "            rec.autos       0.79      0.76      0.77       396\n",
      "   rec.sport.baseball       0.70      0.82      0.76       397\n",
      "     rec.sport.hockey       0.88      0.88      0.88       399\n",
      "            sci.crypt       0.85      0.79      0.82       396\n",
      "      sci.electronics       0.70      0.63      0.66       393\n",
      "\n",
      "          avg / total       0.79      0.79      0.79      3151\n",
      "\n",
      "col_0    0    1    2    3    4    5    6    7\n",
      "row_0                                        \n",
      "0      295   13   26   11    7    3    8   42\n",
      "1        8  332    0    3    4    4    4   14\n",
      "2       18    7  322   18    7    0   14   28\n",
      "3        8    5   11  299   11   10   13   21\n",
      "4       17   13   14   35  327   19   22   17\n",
      "5        4    2    4    6   25  352    6    2\n",
      "6       11    5    2    4    7    5  311   22\n",
      "7       24   18   11   20    9    6   18  247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Start timing\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Create instance and fit\n",
    "sgdc = SGDClassifier(loss = 'hinge')\n",
    "sgdc.fit(X_train, y)\n",
    "\n",
    "# Apply to testing data\n",
    "y_hat = sgdc.predict(X_test)\n",
    "\n",
    "# Stop timing\n",
    "stop = timeit.default_timer()\n",
    "sgd_time = stop - start\n",
    "print(\"Run time: %0.3f\" % sgd_time)\n",
    "\n",
    "# Showing model performance\n",
    "cross = pd.crosstab(y_hat, y_test)\n",
    "print(\"Accuracy is: %0.3f\" % sgdc.score(X_test, y_test))\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))\n",
    "print(cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=1, fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n",
      "       loss='log', n_iter=5, n_jobs=1, penalty='l2', power_t=1.5,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "Accuracy is:  0.8064106632814979\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "comp.sys.mac.hardware       0.79      0.77      0.78       385\n",
      "       comp.windows.x       0.90      0.86      0.88       395\n",
      "         misc.forsale       0.82      0.81      0.81       390\n",
      "            rec.autos       0.79      0.79      0.79       396\n",
      "   rec.sport.baseball       0.70      0.87      0.78       397\n",
      "     rec.sport.hockey       0.90      0.89      0.89       399\n",
      "            sci.crypt       0.87      0.79      0.83       396\n",
      "      sci.electronics       0.71      0.66      0.69       393\n",
      "\n",
      "          avg / total       0.81      0.81      0.81      3151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'loss': ['hinge', 'log'],\n",
    "          'penalty': ['l1', 'l2'],\n",
    "          'alpha': [0.0001, 0.00001, 0.00000001],\n",
    "          'average': [True, False],\n",
    "          'class_weight': ['balanced', None],\n",
    "          'learning_rate':['optimal', 'invscaling'],\n",
    "          # Tried 0.5 in previous grid\n",
    "          'power_t': [1.5],\n",
    "          'eta0': [1],\n",
    "          'n_iter': [5, 100]\n",
    "          #'tol': [0.001, 0.0001],\n",
    "         }\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "sgdc = SGDClassifier()\n",
    "\n",
    "# Apply GridSearch to the model\n",
    "grid = model_selection.GridSearchCV(sgdc, params)\n",
    "grid.fit(X_train, y)\n",
    "\n",
    "# Save instance for CV\n",
    "sgd_best = grid.best_estimator_\n",
    "print(grid.best_estimator_)\n",
    "\n",
    "# Metrics\n",
    "print(\"Accuracy is: \", grid.score(X_test, y_test))\n",
    "\n",
    "y_hat = grid.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_hat,\n",
    "                                    target_names=dataset_test.target_names))\n",
    "cross = pd.crosstab(y_hat, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "# Time, accuracy, precision, recall, f1\n",
    "compareModels['SGD'] = [sgd_time, metrics.accuracy_score(y_test, y_hat),\n",
    "                     metrics.precision_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.recall_score(y_test, y_hat, average = 'macro'),\n",
    "                     metrics.f1_score(y_test, y_hat, average = 'macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for iteration 0 is 0.8394216133942162\n",
      "Score for iteration 1 is 0.8584474885844748\n",
      "Score for iteration 2 is 0.8447488584474886\n",
      "Score for iteration 3 is 0.8318112633181126\n",
      "Score for iteration 4 is 0.852359208523592\n",
      "Score for iteration 5 is 0.845509893455099\n"
     ]
    }
   ],
   "source": [
    "# Calculating CV using KFolds\n",
    "count = 0\n",
    "kf = model_selection.KFold(n_splits=6, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(news_full, y_full):    \n",
    "    Xk_train = vectorizer.fit_transform(news_full.iloc[train_index,0])\n",
    "    Xk_test =  vectorizer.transform(news_full.iloc[test_index, 0])\n",
    "    yk_train, yk_test = y_full[train_index], y_full[test_index]\n",
    "    # Create instance based on GridCV\n",
    "    sgd = sgd_best\n",
    "    sgd.fit(Xk_train, yk_train)\n",
    "    print('Score for iteration {} is {}'.format(count, sgd.score(Xk_test, yk_test)))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Run time  Accuracy  Precision  Recall    f1\n",
      "nb       0.03      0.82       0.82    0.82  0.82\n",
      "SVC     19.06      0.78       0.79    0.78  0.79\n",
      "XGC     19.53      0.74       0.75    0.74  0.74\n",
      "SGD      0.18      0.81       0.81    0.81  0.81\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame.from_dict(compareModels, orient='index')\n",
    "summary.columns = ['Run time', 'Accuracy', 'Precision', 'Recall', 'f1']\n",
    "print(summary.round(2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly the Naïve Bayes classifier performed best. It has the highest metrics and the model runs orders of magnitude faster than the more complex models. It is possible that better performance could be obtained from the extreme gradient descent and stochastic gradient descent models by allowing more iterations (for example the optimized xgboost model used n_estimators = 1000 – which was the maximum option fed into Gridsearch). However, this would slow down the run time performance even further and gains are likely to be limited (in the xgboost example the final accuracy was only marginally improved going from n_estimators=10 to n_estimators=1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
