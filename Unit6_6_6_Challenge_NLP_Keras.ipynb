{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Building neural networks\n",
    "\n",
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make different implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset:** I build on the work I already did for the 20 newsgroups dataset. It is a text-based dataset built into the scikit learn library comprises around 18000 newsgroups posts on 20 topics. The main page and instructions for downloading can be found [here]( http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n",
    "\n",
    "By using this dataset I can compare the results of various supervised algorithms to the results of using various neural networks.\n",
    "\n",
    "The highest accuracy was with a Naive Bayes model (82%) and a speedy runtime of 0.03s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I apply three different models to the dataset.\n",
    "\n",
    "1. **Multi-layer perceptron with tf-idf**: This model performs relatively well. Running with epochs = 15 it obtains an accuracy of 81% - already close to the highest accuracy of Naive Bayes model. While further gains could be extracted by optimizing the model (no gridsearch approach is taken here), the biggest cost of shifting into a neural net is the run time of the model. A single epoch takes orders of magnitute longer to run (30s) than a Naive Bayes (0.03s) and even a standard SDG model (6s).\n",
    "\n",
    "2. **Multi-layer perceptron using embedding**: Directly using an embedding layer in the neural network (rather than simple inputting a tf-idf matrix) did not improve the model. In order to keep the number of parameters smaller, the feature set is reduced and only some of the words are sampled. This, I think, leads to information loss (particularly in the padding process compared to creating tf-idf matrix).   \n",
    "\n",
    "3. ** Recurrent neural network**: The RNN model does surprisingly poorly (45% accuracy). This is likely due to the setup (particularly the restricting of words and condensing the feature set). However, with a similar set-up the MLP still obtains 75% accuracy.\n",
    "\n",
    "4. **Long short-term memory with word2vec as input**: This model is interesting as it uses vectors created by the word2vec process as an input into another neural network - in this case LSTM. Using word2vec should improve on the bag of words/ tf-idf approach as the vectors created by word2vec retains more information. It should also solve some of the padding/ embedding issues. However, it takes too long to run (over an hour per epoch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "\n",
    "This notebook is highly indebted to two sources:\n",
    "\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb\n",
    "\n",
    "https://github.com/giuseppebonaccorso/Reuters-21578-Classification/blob/master/Text%20Classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "\n",
    "# Full dataset categories\n",
    "categories = ['comp.graphics','comp.os.ms-windows.misc',\n",
    "                  'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "                  'comp.windows.x', 'rec.autos','rec.motorcycles',\n",
    "                  'rec.sport.baseball','rec.sport.hockey', 'sci.crypt',\n",
    "                  'sci.electronics','sci.med','sci.space',\n",
    "                  'misc.forsale', 'talk.politics.misc',\n",
    "                  'talk.politics.guns','talk.politics.mideast', 'talk.religion.misc',\n",
    "                  'alt.atheism','soc.religion.christian']\n",
    "\n",
    "# Reduce size of dataset to improve speed\n",
    "random.seed(13)\n",
    "categories_small = random.sample(categories, 8)\n",
    "\n",
    "# Import dataset (type Bunch)\n",
    "# Remove headers, footers, and quotes so that classifiers only work on text\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories_small,\n",
    "                             remove=('headers', 'footers', 'quotes'),\n",
    "                             shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "dataset_test = fetch_20newsgroups(subset='test', categories=categories_small,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "# Also import full dataset for cv\n",
    "dataset_full = fetch_20newsgroups(subset='all', categories=categories_small,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "# Convert to dataframe\n",
    "news = pd.DataFrame(dataset.data, columns=['Text'])\n",
    "news_test = pd.DataFrame(dataset_test.data, columns=['Text'])\n",
    "news_full = pd.DataFrame(dataset_full.data, columns=['Text'])\n",
    "\n",
    "# Set outcome variable\n",
    "y = dataset.target\n",
    "y_test = dataset_test.target\n",
    "y_full = dataset_full.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Basic stopwords list\n",
    "stopWords = stopwords.words('english')\n",
    "stopped = []\n",
    "# Change I'll to Ill\n",
    "for w in stopWords:\n",
    "    stopped.append(re.sub(r\"\\'\", '', w))\n",
    "\n",
    "def textcleaner(text):\n",
    "    ''' Takes in raw unformatted text and strips punctuation, removes whitespace,\n",
    "    strips numbers, tokenizes and stems.\n",
    "    Returns string of processed text to be used into CountVectorizer\n",
    "    '''\n",
    "    # Lowercase and strip everything except words\n",
    "    cleaner = re.sub(r\"[^a-zA-Z ]+\", ' ', text.lower())\n",
    "    # Tokenize\n",
    "    cleaner = word_tokenize(cleaner)\n",
    "    ps = PorterStemmer()\n",
    "    clean = []\n",
    "    for w in cleaner:\n",
    "        # filter out stopwords\n",
    "        if w not in stopped:\n",
    "            # filter out short words\n",
    "            if len(w)>2:\n",
    "                # Stem \n",
    "                clean.append(ps.stem(w))\n",
    "    return ' '.join(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done news\n",
      "Done test\n",
      "Done full\n"
     ]
    }
   ],
   "source": [
    "# Clean up the dfs\n",
    "news['Clean_text'] = news.Text.apply(lambda x: textcleaner(x))\n",
    "print('Done news')\n",
    "news_test['Clean_text'] = news_test.Text.apply(lambda x: textcleaner(x))\n",
    "print('Done test')\n",
    "news_full['Clean_text'] = news_full.Text.apply(lambda x: textcleaner(x))\n",
    "print('Done full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop unprocessed text\n",
    "news.drop(['Text'], inplace=True, axis=1)\n",
    "news_test.drop(['Text'], inplace=True, axis=1)\n",
    "news_full.drop(['Text'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data into correct form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 31059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Counting number of words\n",
    "count_vec = CountVectorizer(strip_accents='ascii', ngram_range=(1, 1), \n",
    "                                analyzer='word',  stop_words='english')\n",
    "count_fit = count_vec.fit_transform(news_full.loc[:, 'Clean_text'])\n",
    "#Number of columns = number of unique words\n",
    "print(\"Number of unique words:\", count_fit.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Clean_text\n",
      "0  make ten eight met astro join\n",
      "[15, 1383, 2467, 1205, 2337, 1965]\n"
     ]
    }
   ],
   "source": [
    "# Can use num_words to reduce size to speed up model\n",
    "num_words = count_fit.shape[1]\n",
    "\n",
    "# Tokenize words (a bit of duplication here from original cleaning...)\n",
    "# The +1 is because I'm not using the embedding layer (which masks on index 0)\n",
    "# See https://github.com/keras-team/keras/issues/8583\n",
    "tokenizer = Tokenizer(num_words = (num_words + 1))\n",
    "tokenizer.fit_on_texts(news_full.loc[:,'Clean_text'])\n",
    "\n",
    "# Tokenizer is a dict map words to index\n",
    "\n",
    "# Convert words in sample to index\n",
    "x_train_tokens = tokenizer.texts_to_sequences(news.loc[:, 'Clean_text'])\n",
    "x_test_tokens = tokenizer.texts_to_sequences(news_test.loc[:, 'Clean_text'])\n",
    "\n",
    "# Compare...\n",
    "print(news.iloc[0:1])\n",
    "# ...to\n",
    "print((x_train_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Clean_text\n",
      "0  make ten eight met astro join\n",
      "[15, 1383, 2467, 1205, 2337, 1965]\n",
      "6\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(4733, 31060)\n"
     ]
    }
   ],
   "source": [
    "# Currently samples are of different length, move into matrix\n",
    "# Use built-in tfidf transformation\n",
    "X_train_matrix = tokenizer.texts_to_matrix(news.loc[:, 'Clean_text'], mode='tfidf')\n",
    "X_test_matrix = tokenizer.texts_to_matrix(news_test.loc[:, 'Clean_text'], mode='tfidf')\n",
    "\n",
    "# Compare...\n",
    "print(news.iloc[0:1])\n",
    "# ...to\n",
    "print((x_train_tokens[0]))\n",
    "print(len(x_train_tokens[0]))\n",
    "#... to\n",
    "print(X_train_matrix[0])\n",
    "print(X_train_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4733, 8)\n",
      "(3151, 8)\n"
     ]
    }
   ],
   "source": [
    "# One hot encoding for y\n",
    "y_train_long = keras.utils.to_categorical(y, 8)\n",
    "print(y_train_long.shape)\n",
    "y_test_long = keras.utils.to_categorical(y_test, 8)\n",
    "print(y_test_long.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               7951360   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 7,985,288\n",
      "Trainable params: 7,985,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4733 samples, validate on 3151 samples\n",
      "Epoch 1/15\n",
      "4733/4733 [==============================] - 53s 11ms/step - loss: 1.0603 - acc: 0.7097 - val_loss: 0.6285 - val_acc: 0.8147\n",
      "Epoch 2/15\n",
      "4733/4733 [==============================] - 34s 7ms/step - loss: 0.2228 - acc: 0.9446 - val_loss: 0.5997 - val_acc: 0.8261\n",
      "Epoch 3/15\n",
      "4733/4733 [==============================] - 31s 7ms/step - loss: 0.1155 - acc: 0.9715 - val_loss: 0.6118 - val_acc: 0.8258\n",
      "Epoch 4/15\n",
      "4733/4733 [==============================] - 30s 6ms/step - loss: 0.0872 - acc: 0.9736 - val_loss: 0.6433 - val_acc: 0.8194\n",
      "Epoch 5/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0780 - acc: 0.9751 - val_loss: 0.6690 - val_acc: 0.8185\n",
      "Epoch 6/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0716 - acc: 0.9757 - val_loss: 0.6711 - val_acc: 0.8178\n",
      "Epoch 7/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0732 - acc: 0.9749 - val_loss: 0.7123 - val_acc: 0.8166\n",
      "Epoch 8/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0688 - acc: 0.9759 - val_loss: 0.7345 - val_acc: 0.8169\n",
      "Epoch 9/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0671 - acc: 0.9763 - val_loss: 0.7218 - val_acc: 0.8153\n",
      "Epoch 10/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0671 - acc: 0.9770 - val_loss: 0.7538 - val_acc: 0.8131\n",
      "Epoch 11/15\n",
      "4733/4733 [==============================] - 29s 6ms/step - loss: 0.0668 - acc: 0.9749 - val_loss: 0.7700 - val_acc: 0.8124\n",
      "Epoch 12/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0668 - acc: 0.9761 - val_loss: 0.7821 - val_acc: 0.8140\n",
      "Epoch 13/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0657 - acc: 0.9761 - val_loss: 0.7774 - val_acc: 0.8124\n",
      "Epoch 14/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0678 - acc: 0.9759 - val_loss: 0.7881 - val_acc: 0.8118\n",
      "Epoch 15/15\n",
      "4733/4733 [==============================] - 28s 6ms/step - loss: 0.0674 - acc: 0.9770 - val_loss: 0.8123 - val_acc: 0.8096\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(num_words,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting model (note adjusting matrix to avoid 0)\n",
    "history = model.fit(X_train_matrix[:,1:], y_train_long,\n",
    "                    batch_size=128,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test_matrix[:, 1:], y_test_long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3151/3151 [==============================] - 7s 2ms/step\n",
      "Test loss: 0.8122931501669416\n",
      "Test accuracy: 0.80958425900324\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test_matrix[:, 1:], y_test_long, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "all_scores['mlp_single'] = score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer perceptron with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.00634195839675\n",
      "6735\n",
      "573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4733, 573)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize words (a bit of duplication here from original cleaning...)\n",
    "# Note no longer num_words +1, use max_words here to avoid confusion with above\n",
    "num_words = 20000\n",
    "tokenizer = Tokenizer(num_words = num_words)\n",
    "tokenizer.fit_on_texts(news_full.loc[:,'Clean_text'])\n",
    "\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(news.loc[:, 'Clean_text'])\n",
    "x_test_tokens = tokenizer.texts_to_sequences(news_test.loc[:, 'Clean_text'])\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(np.mean(num_tokens))\n",
    "print(np.max(num_tokens))\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(max_tokens)\n",
    "\n",
    "# Pad the samples to get them to equal length\n",
    "pad = 'pre'\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "x_train_pad.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 573, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 57300)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               7334528   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 9,343,304\n",
      "Trainable params: 9,343,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "embedding_size = 100\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', input_shape=(embedding_size,)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4733 samples, validate on 3151 samples\n",
      "Epoch 1/10\n",
      "4733/4733 [==============================] - 40s 8ms/step - loss: 2.1153 - acc: 0.1247 - val_loss: 2.0719 - val_acc: 0.1295\n",
      "Epoch 2/10\n",
      "4733/4733 [==============================] - 35s 7ms/step - loss: 1.9758 - acc: 0.2066 - val_loss: 1.8253 - val_acc: 0.2653\n",
      "Epoch 3/10\n",
      "4733/4733 [==============================] - 33s 7ms/step - loss: 1.5115 - acc: 0.3746 - val_loss: 1.4160 - val_acc: 0.4186\n",
      "Epoch 4/10\n",
      "4733/4733 [==============================] - 33s 7ms/step - loss: 0.9097 - acc: 0.6772 - val_loss: 0.8718 - val_acc: 0.7055\n",
      "Epoch 5/10\n",
      "4733/4733 [==============================] - 33s 7ms/step - loss: 0.3365 - acc: 0.9007 - val_loss: 0.7327 - val_acc: 0.7582\n",
      "Epoch 6/10\n",
      "4733/4733 [==============================] - 34s 7ms/step - loss: 0.1578 - acc: 0.9550 - val_loss: 0.7538 - val_acc: 0.7639\n",
      "Epoch 7/10\n",
      "4733/4733 [==============================] - 33s 7ms/step - loss: 0.1067 - acc: 0.9694 - val_loss: 0.7927 - val_acc: 0.7604\n",
      "Epoch 8/10\n",
      "4733/4733 [==============================] - 33s 7ms/step - loss: 0.0925 - acc: 0.9713 - val_loss: 0.7965 - val_acc: 0.7591\n",
      "Epoch 9/10\n",
      "4733/4733 [==============================] - 36s 8ms/step - loss: 0.0796 - acc: 0.9730 - val_loss: 0.8185 - val_acc: 0.7582\n",
      "Epoch 10/10\n",
      "4733/4733 [==============================] - 35s 7ms/step - loss: 0.0775 - acc: 0.9742 - val_loss: 0.8587 - val_acc: 0.7544\n",
      "3151/3151 [==============================] - 6s 2ms/step\n",
      "Test loss: 0.858665461626858\n",
      "Test accuracy: 0.7543636940842922\n"
     ]
    }
   ],
   "source": [
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting model (note adjusting matrix to avoid 0)\n",
    "history = model.fit(x_train_pad, y_train_long,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_pad, y_test_long))\n",
    "\n",
    "score = model.evaluate(x_test_pad, y_test_long, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores['mlp_embed'] = score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 573, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 573, 16)           5616      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 573, 8)            600       \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 40        \n",
      "=================================================================\n",
      "Total params: 2,006,412\n",
      "Trainable params: 2,006,412\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "#embedding_size = 100\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4733 samples, validate on 3151 samples\n",
      "Epoch 1/10\n",
      "4733/4733 [==============================] - 121s 26ms/step - loss: 1.9990 - acc: 0.1988 - val_loss: 1.8906 - val_acc: 0.2199\n",
      "Epoch 2/10\n",
      "4733/4733 [==============================] - 110s 23ms/step - loss: 1.7520 - acc: 0.2402 - val_loss: 1.7819 - val_acc: 0.2263\n",
      "Epoch 3/10\n",
      "4733/4733 [==============================] - 98s 21ms/step - loss: 1.6296 - acc: 0.2969 - val_loss: 1.7298 - val_acc: 0.2755\n",
      "Epoch 4/10\n",
      "4733/4733 [==============================] - 95s 20ms/step - loss: 1.4966 - acc: 0.4198 - val_loss: 1.6472 - val_acc: 0.3364\n",
      "Epoch 5/10\n",
      "4733/4733 [==============================] - 100s 21ms/step - loss: 1.3770 - acc: 0.5434 - val_loss: 1.6193 - val_acc: 0.3938\n",
      "Epoch 6/10\n",
      "4733/4733 [==============================] - 98s 21ms/step - loss: 1.2821 - acc: 0.6174 - val_loss: 1.5672 - val_acc: 0.4332\n",
      "Epoch 7/10\n",
      "4733/4733 [==============================] - 99s 21ms/step - loss: 1.2052 - acc: 0.6924 - val_loss: 1.5211 - val_acc: 0.4621\n",
      "Epoch 8/10\n",
      "4733/4733 [==============================] - 100s 21ms/step - loss: 1.1428 - acc: 0.7532 - val_loss: 1.5057 - val_acc: 0.4526\n",
      "Epoch 9/10\n",
      "4733/4733 [==============================] - 97s 21ms/step - loss: 1.0886 - acc: 0.7262 - val_loss: 1.5054 - val_acc: 0.4541\n",
      "Epoch 10/10\n",
      "4733/4733 [==============================] - 97s 20ms/step - loss: 1.0402 - acc: 0.7405 - val_loss: 1.4728 - val_acc: 0.4583\n",
      "3151/3151 [==============================] - 28s 9ms/step\n",
      "Test loss: 1.472765001105112\n",
      "Test accuracy: 0.45826721678495935\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_pad, y_train_long,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test_pad, y_test_long))\n",
    "\n",
    "score = model.evaluate(x_test_pad, y_test_long, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores['rnn'] = score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using tf-idf matrix rather than embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = count_fit.shape[1]\n",
    "\n",
    "# Tokenize words (a bit of duplication here from original cleaning...)\n",
    "# The +1 is because I'm not using the embedding layer (which masks on index 0)\n",
    "# See https://github.com/keras-team/keras/issues/8583\n",
    "tokenizer = Tokenizer(num_words = (num_words + 1))\n",
    "tokenizer.fit_on_texts(news_full.loc[:,'Clean_text'])\n",
    "\n",
    "# Tokenizer is a dict map words to index\n",
    "\n",
    "X_train_matrix = tokenizer.texts_to_matrix(news.loc[:, 'Clean_text'], mode='tfidf')\n",
    "X_test_matrix = tokenizer.texts_to_matrix(news_test.loc[:, 'Clean_text'], mode='tfidf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change to 3D\n",
    "X_train_matrix3 = np.reshape(X_train_matrix, (len(X_train_matrix), num_words+1, 1))\n",
    "\n",
    "model = Sequential()\n",
    "#embedding_size = 100\n",
    "model.add(SimpleRNN(units=32, return_sequences=True, input_shape=(num_words+1, 1)))\n",
    "model.add(SimpleRNN(units=16, return_sequences=True))\n",
    "model.add(SimpleRNN(units=8))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DANGER: This takes hours to run... Not attempted here\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting model (note adjusting matrix to avoid 0)\n",
    "history = model.fit(X_train_matrix3, y_train_long,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    #validation_data=(X_test_matrix[:, 1:], y_test_long)\n",
    "                   )\n",
    "\n",
    "score = model.evaluate(X_test_matrix[:, 1:], y_test_long, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_scores['rnn_tf_idf'] = score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short-term memory (LSTM) with Word2vec as input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim model expects tokens\n",
    "X_train = news.Clean_text.apply(lambda x: x.split(sep=' '))\n",
    "X_test = news_test.Clean_text.apply(lambda x: x.split(sep=' '))\n",
    "X_full = news_full.Clean_text.apply(lambda x: x.split(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# Word2Vec number of features\n",
    "num_features =100\n",
    "\n",
    "# Creating instance\n",
    "w2v_model = word2vec.Word2Vec(\n",
    "    X_full,\n",
    "    #workers=4,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    #min_count=1,  # Minimum word count threshold.\n",
    "    window=10,      # Number of words around target word to consider.\n",
    "    #sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=num_features,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = dict(zip(w2v_model.wv.index2word, w2v_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit each newsline to a fixed number of words\n",
    "document_max_num_words = max_tokens\n",
    "\n",
    "# Categories (should really refer to start rather than hardcoded)\n",
    "num_categories = 8\n",
    "\n",
    "# Creating vars to match code below\n",
    "number_of_documents = len(news_full)\n",
    "\n",
    "X_wv = np.zeros(shape=(number_of_documents, document_max_num_words, num_features)).astype(np.float32)\n",
    "\n",
    "empty_word = np.zeros(num_features).astype(np.float32)\n",
    "\n",
    "for idx, document in enumerate(news_full.loc[:, 'Clean_text']):\n",
    "    for jdx, word in enumerate(document):\n",
    "        if jdx == document_max_num_words:\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            if word in w2v_model:\n",
    "                X_wv[idx, jdx, :] = w2v_model[word]\n",
    "            else:\n",
    "                X_wv[idx, jdx, :] = empty_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7884, 573, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7884, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_wv_long = keras.utils.to_categorical(y_full, 8)\n",
    "Y_wv_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_wv, X_test_wv, y_train_wv, y_test_wv = train_test_split(X_wv, Y_wv_long, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 630)               1842120   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 630)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 5048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,847,168\n",
      "Trainable params: 1,847,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(int(document_max_num_words*1.1), input_shape=(document_max_num_words, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 448/6307 [=>............................] - ETA: 1:03:52 - loss: 2.0805 - acc: 0.1094"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-277e7a2381b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_wv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_wv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# An hour is a bit long to wait...\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_wv, y_train_wv, batch_size=64, epochs=5)\n",
    "\n",
    "# Evaluate model\n",
    "score, acc = model.evaluate(X_test_wv, y_test_wv, batch_size=128)\n",
    "    \n",
    "print('Score: %1.4f' % score)\n",
    "print('Accuracy: %1.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
